{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyhK9-HaTejF"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting out with some basic setup to simplify the walk-through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bUPM0-8iLNK0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def embedding(word):\n",
    "    return np.array(df.loc[df[\"word\"] == word, \"embedding\"].item())\n",
    "\n",
    "def calculate_cosine_similarity(a, b):\n",
    "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def find_similar_concepts(query_embedding, top_n=5, filter=[]):\n",
    "    df[\"similarity\"] = df[\"embedding\"].apply(lambda emb: calculate_cosine_similarity(query_embedding, emb))\n",
    "    result = df[~df[\"word\"].isin(filter)].copy()\n",
    "    return result.sort_values(\"similarity\", ascending=False).head(top_n).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding(text):\n",
    "  return np.array(client.embeddings.create(input=[text.replace(\"\\n\", \" \")], model=\"text-embedding-3-small\").data[0].embedding)\n",
    "\n",
    "words = [\n",
    "    \"man\", \"woman\", \"king\", \"queen\", \"boy\", \"girl\", \"prince\", \"princess\",\n",
    "    \"father\", \"mother\", \"husband\", \"wife\", \"son\", \"daughter\", \"friend\",\n",
    "    \"enemy\", \"teacher\", \"student\", \"boss\", \"employee\", \"doctor\", \"nurse\",\n",
    "    \"potatoes\", \"cheese\", \"hamburger\", \"french fries\", \"cheeseburger\",\n",
    "    \"banana\", \"chocolate\", \"cake\", \"cheesecake\", \"bacon\", \"pancakes\",\n",
    "    \"avocado\", \"salad\", \"peanut butter\", \"toast\", \"burrito\", \"sushi\",\n",
    "    \"pizza\", \"ice cream\", \"cookie\", \"hot dog\",\n",
    "    \"soda\", \"water\", \"milk\", \"la croix\", \"coffee\", \"latte\", \"espresso\",\n",
    "    \"mocha\", \"tea\", \"hot chocolate\", \"cappuccino\", \"matcha\", \"smoothie\",\n",
    "    \"kombucha\", \"wine\", \"beer\", \"cocktail\", \"lemonade\",\n",
    "    \"red\", \"blue\", \"green\", \"yellow\", \"brown\", \"black\", \"white\", \"orange\",\n",
    "    \"pink\", \"purple\", \"beige\", \"gray\", \"neon\", \"clear\", \"golden\",\n",
    "    \"crispy\", \"fizzy\", \"sweet\", \"salty\", \"creamy\", \"hot\", \"cold\", \"spicy\",\n",
    "    \"refreshing\", \"bitter\", \"sour\", \"smooth\", \"crunchy\", \"bubbly\", \"strong\",\n",
    "    \"light\", \"dark\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"word\": words,\n",
    "    \"embedding\": [calculate_embedding(word) for word in words],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9rqNdHIT3tB"
   },
   "source": [
    "# Embedding Concepts in Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTU3q5OV6nV8"
   },
   "source": [
    "Embeddings are a technique used in natural language processing (NLP) to represent words, phrases, and even larger blocks of text as vectors of numbers in a high-dimensional space.\n",
    "\n",
    "* Each vector can be thought of as a coordinate in this high-dimensional space.\n",
    "* Each dimension in the vector captures an aspect of semantic meaning.\n",
    "* The structure of this space reflects learned relationships from training data.\n",
    "\n",
    "For example, the idea of a 'hot dog' is represented by a vector. That means that the model has learned to locate the concept of \"hot dog\" at specific coordinates in its semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WnRB_pM-Sy3j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03104307, -0.04807128, -0.01409304, ..., -0.00020377,\n",
       "        0.0029014 , -0.0072846 ], shape=(1536,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(\"hot dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfWfJINX9B3g"
   },
   "source": [
    "Where GPS has two dimensions (latitude and longitude), and physical space has three (x, y, z), the model we're using represents concepts in **many** dimensions—often hundreds or thousands. This is what we mean by a \"high-dimensional space.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxHx2j0l4Mnb",
    "outputId": "8e2c5bb7-1566-4312-b8e8-9d8350859d38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding(\"hot dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jq9UG9uA4Mx"
   },
   "source": [
    "# Comparing Similarity by Measuring Direction\n",
    "\n",
    "A key capability in this space is measuring similarity. One technique is **cosine similarity**. Instead of measuring literal distance, cosine similarity measures the **angle** between two vectors:\n",
    "\n",
    "These values are normalized in a range from -1 to 1:\n",
    "* 1 indicates that the vectors are in the same direction (meaning very similar)\n",
    "* 0 indicates that they are orthogonal or have no similarity\n",
    "* -1 indicates that they are in exactly opposite directions (meaning they are dissimilar)\n",
    "\n",
    "This allows us to infer semantic relationships based on direction — such as meaning, context, or usage.\n",
    "\n",
    "So in this example, we’ll ask: *What example words are most similar to the word \"burrito\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 855
    },
    "id": "BhE3ATG80oAt",
    "outputId": "45032164-f073-4a88-bfc9-36ee5198fbdf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cheeseburger</td>\n",
       "      <td>[-0.025503935292363167, -0.05670874938368797, ...</td>\n",
       "      <td>0.512808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hamburger</td>\n",
       "      <td>[-0.046442754566669464, -0.056426260620355606,...</td>\n",
       "      <td>0.476006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hot dog</td>\n",
       "      <td>[-0.031043073162436485, -0.048071280121803284,...</td>\n",
       "      <td>0.433672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bacon</td>\n",
       "      <td>[0.027432523667812347, -0.007462325040251017, ...</td>\n",
       "      <td>0.423240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sushi</td>\n",
       "      <td>[-0.023778622969985008, -0.025500649586319923,...</td>\n",
       "      <td>0.408351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word                                          embedding  similarity\n",
       "0  cheeseburger  [-0.025503935292363167, -0.05670874938368797, ...    0.512808\n",
       "1     hamburger  [-0.046442754566669464, -0.056426260620355606,...    0.476006\n",
       "2       hot dog  [-0.031043073162436485, -0.048071280121803284,...    0.433672\n",
       "3         bacon  [0.027432523667812347, -0.007462325040251017, ...    0.423240\n",
       "4         sushi  [-0.023778622969985008, -0.025500649586319923,...    0.408351"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_concepts(embedding(\"burrito\"), filter=[\"burrito\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8xPhtZRC6S5"
   },
   "source": [
    "# Emergent Meaning through Vector Arithmatic\n",
    "\n",
    "One of the most fascinating aspects of this semantic space is that we can not only measure similarity — we can **perform math on meaning**.\n",
    "\n",
    "By adding, subtracting, and blending vectors, we can explore entirely new conceptual combinations.\n",
    "\n",
    "Take this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 855
    },
    "id": "s3UCdDmJjfaQ",
    "outputId": "1b541b93-741a-4370-c2bf-e3dad34970c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cappuccino</td>\n",
       "      <td>[-0.02482164278626442, -0.030885322019457817, ...</td>\n",
       "      <td>0.642723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latte</td>\n",
       "      <td>[-0.017921417951583862, -0.03153694421052933, ...</td>\n",
       "      <td>0.577750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mocha</td>\n",
       "      <td>[0.02025661990046501, -0.026742229238152504, -...</td>\n",
       "      <td>0.562984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>[0.013673103414475918, -0.04669247195124626, 0...</td>\n",
       "      <td>0.536867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hot chocolate</td>\n",
       "      <td>[-0.04974162578582764, -0.04538385942578316, -...</td>\n",
       "      <td>0.518881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word                                          embedding  \\\n",
       "0     cappuccino  [-0.02482164278626442, -0.030885322019457817, ...   \n",
       "1          latte  [-0.017921417951583862, -0.03153694421052933, ...   \n",
       "2          mocha  [0.02025661990046501, -0.026742229238152504, -...   \n",
       "3      chocolate  [0.013673103414475918, -0.04669247195124626, 0...   \n",
       "4  hot chocolate  [-0.04974162578582764, -0.04538385942578316, -...   \n",
       "\n",
       "   similarity  \n",
       "0    0.642723  \n",
       "1    0.577750  \n",
       "2    0.562984  \n",
       "3    0.536867  \n",
       "4    0.518881  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_concepts(embedding(\"milk\") + embedding(\"espresso\"), filter=[\"milk\", \"espresso\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mother</td>\n",
       "      <td>[0.06384002417325974, 0.002675893483683467, -0...</td>\n",
       "      <td>0.747128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daughter</td>\n",
       "      <td>[0.07486723363399506, -0.017612170428037643, -...</td>\n",
       "      <td>0.610720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wife</td>\n",
       "      <td>[0.017703169956803322, 0.0015181683702394366, ...</td>\n",
       "      <td>0.579919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>queen</td>\n",
       "      <td>[0.043817322701215744, -0.03984493762254715, 0...</td>\n",
       "      <td>0.423459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>friend</td>\n",
       "      <td>[-0.017377346754074097, -0.03304498642683029, ...</td>\n",
       "      <td>0.411813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                          embedding  similarity\n",
       "0    mother  [0.06384002417325974, 0.002675893483683467, -0...    0.747128\n",
       "1  daughter  [0.07486723363399506, -0.017612170428037643, -...    0.610720\n",
       "2      wife  [0.017703169956803322, 0.0015181683702394366, ...    0.579919\n",
       "3     queen  [0.043817322701215744, -0.03984493762254715, 0...    0.423459\n",
       "4    friend  [-0.017377346754074097, -0.03304498642683029, ...    0.411813"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_concepts(embedding(\"father\") - embedding(\"man\") + embedding(\"woman\"), filter=[\"father\", \"man\", \"woman\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emergent Meaning through Phrases\n",
    "\n",
    "Instead of combining known vectors, we can also embed full **natural language phrases** directly.\n",
    "\n",
    "This allows us to describe concepts in our own words — and let the model retrieve similar ideas based on learned meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ice cream</td>\n",
       "      <td>[0.015797361731529236, -0.0515950508415699, -0...</td>\n",
       "      <td>0.449068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>[0.04396495223045349, -0.02441263012588024, -0...</td>\n",
       "      <td>0.435389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smoothie</td>\n",
       "      <td>[-0.022336609661579132, -0.034296080470085144,...</td>\n",
       "      <td>0.403105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cake</td>\n",
       "      <td>[0.04641443118453026, -0.009220455773174763, -...</td>\n",
       "      <td>0.396299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>creamy</td>\n",
       "      <td>[0.049029696732759476, -0.008617117069661617, ...</td>\n",
       "      <td>0.378161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word                                          embedding  similarity\n",
       "0   ice cream  [0.015797361731529236, -0.0515950508415699, -0...    0.449068\n",
       "1  cheesecake  [0.04396495223045349, -0.02441263012588024, -0...    0.435389\n",
       "2    smoothie  [-0.022336609661579132, -0.034296080470085144,...    0.403105\n",
       "3        cake  [0.04641443118453026, -0.009220455773174763, -...    0.396299\n",
       "4      creamy  [0.049029696732759476, -0.008617117069661617, ...    0.378161"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_concepts(calculate_embedding(\"dessert you eat with a spoon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>father</td>\n",
       "      <td>[0.02880435809493065, 0.024050738662481308, 0....</td>\n",
       "      <td>0.469758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daughter</td>\n",
       "      <td>[0.07486723363399506, -0.017612170428037643, -...</td>\n",
       "      <td>0.445740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mother</td>\n",
       "      <td>[0.06384002417325974, 0.002675893483683467, -0...</td>\n",
       "      <td>0.433022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wife</td>\n",
       "      <td>[0.017703169956803322, 0.0015181683702394366, ...</td>\n",
       "      <td>0.423573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>husband</td>\n",
       "      <td>[-0.019892802461981773, 0.03318863362073898, -...</td>\n",
       "      <td>0.392540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                          embedding  similarity\n",
       "0    father  [0.02880435809493065, 0.024050738662481308, 0....    0.469758\n",
       "1  daughter  [0.07486723363399506, -0.017612170428037643, -...    0.445740\n",
       "2    mother  [0.06384002417325974, 0.002675893483683467, -0...    0.433022\n",
       "3      wife  [0.017703169956803322, 0.0015181683702394366, ...    0.423573\n",
       "4   husband  [-0.019892802461981773, 0.03318863362073898, -...    0.392540"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_concepts(calculate_embedding(\"members of a family\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMUAWRonG7S6"
   },
   "source": [
    "# Take Away\n",
    "\n",
    "A core idea behind large language models (LLMs) is that they are mathematical models of human understanding.\n",
    "\n",
    "Rather than relying on strict logic or fixed rules, these models represent meaning as positions in a high-dimensional space — where the geometry between points reflects relationships between concepts. This allows computers to interpret meaning, not just match literal words.\n",
    "\n",
    "And it doesn’t stop at language. Today’s models are being trained to understand meaning across images, audio, and video — enabling multimodal understanding, a kind of modern-day Rosetta Stone for human perception.\n",
    "\n",
    "I don’t know about you, but as a technologist, I find this to be an incredibly exciting time.\n",
    "\n",
    "After years of working in a world defined by literal, logical, and deterministic systems, we’re now stepping into an era shaped by probabilistic, semantic, and conceptual ones. Just as exciting is how accessible and cost-effective these capabilities have become — not just for researchers, but for everyday developers and creators.\n",
    "\n",
    "The very nature of how we interact with computers is evolving — and it’s happening faster, and at a greater scale, than anything we’ve seen before."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
